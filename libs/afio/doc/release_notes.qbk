[/============================================================================
  Boost.AFIO
  
  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:release_notes Release Notes]

[/=================]
[heading Boost 1.56]
[/=================]

First release.

[endsect]

[section:FAQ Frequently Asked Questions]

[section:closure_performance What is the maximum throughput of AFIO's closure execution engine aka
how many IOPS can I push with AFIO?]

For v1.0, maximum throughput is approximately as follows:

[table:throughput Maximum null closure execution rate on a 3.5Ghz Intel Core i7 3770K for AFIO v1.0:
[[Operating system][`call()` unchained][`call()` chained][`completion()` unchained][`completion()` chained]]
[[Microsoft Windows 8 x64 with Visual Studio 2013][337955][123407][534522][143409]]
[[Ubuntu 12.04 LTS Linux x64 with GCC 4.8][398537][164785][1140640][244907]]
]

While this is fast enough for consumer SSDs, we agree it is currently not great. The biggest limiter, by far, is the
recursive mutex protecting the ops dispatch and completion implementation. Because AFIO is pure batch, dispatch
is very rarely a problem: rather it is completion, because completion handling must always be performed per op completion
in order to check for and dispatch, if necessary, dependencies. Currently the top consumers of CPU
time during null closure dispatch benchmark are as follows:

[table:top_consumers Top consumers of CPU time during null closure dispatch in AFIO v1.0
[[Unchained][][Chained][]]
[[ops Mutex][85%][ops Mutex][99.41%]]
[[threadpool waiting for new ops][4.15%][threadpool waiting for new ops][0.26%]]
[[operator new][2.43%][Remainder and misc][0.33%]]
[[hash table insert][2.05%][][]]
[[sending ops to threadpool][1.67%][][]]
[[Remainder and misc][4.7%][][]]
]

Luckily, the mutex is not held while file i/o operations or closures are being executed, so contention on the mutex
isn't as much of a problem during real world use and therefore is in practice nothing like as bad as things look above
-- in fact, adding in a noop kernel syscall actually slightly improves the figures because the mutex isn't being
contended as heavily, so the above figures for `completion()` are very reasonable max IOPS values. Nevertheless, if you were randomly
reading or writing very small bits of data, we can see how the mutex might get in the way, and we intend to do something
about it in a future v1.x release of __boost_afio__, most especially to eliminate the recursive mutex in favour of a spin lock.

[endsect] [/closure_performance]

[section:stuck_ops I'm seeing ["WARNING: `~async_file_dispatcher_base()` detects stuck `async_io_op` in total of X ops] during
process close. What does this mean?]

This means that you scheduled ops with a dispatcher which did not complete in a timely fashion before you tried to destroy
that dispatcher. This generally indicates a bug in your code, which can include:

# An op is still running in some thread pool and you didn't use __afio_when_all__ to wait for it to complete before trying
to destroy the dispatcher.
# An op's precondition never completed and therefore the op was never started.

Tracking down the cause of the latter in particular is very similar to tracking down race conditions i.e. it's hard, and we
as the authors of __boost_afio__ know just how hard! If you are on POSIX, recompiling AFIO with the macro
`BOOST_AFIO_OP_STACKBACKTRACEDEPTH` set to a reasonable depth like 8 will have AFIO take a stack backtrace for every op
allocated which it then will print during the stuck ops warnings. This can be helpful to identify which ops exactly are
stuck, and then you can figure out which preconditions of theirs are the cause.

Note that `BOOST_AFIO_OP_STACKBACKTRACEDEPTH` has two implementations, one based on glibc `backtrace()` and the other based
on libunwind. The latter is more portable, but requires explicit linking with libunwind, so we have defaulted to the former.

[endsect] [/stuck_ops]

[section:vector_use Why did you use `std::vector<>` as the ops batch container instead of a generic iterator range?]

# `std::vector<async_io_op>` is the closest thing to a variable length array in C++[footnote Ok, there is also
the oft-forgotten `std::valarray<>` too, but its use as a generic container isn't recommended.], at least until C++ 14 where we
will gain `std::dynarray<>` and dynamic array sizing.
# `std::vector<async_io_op>` is well understood, particularly its performance during by-value copies and during
`push_back()` which is by far the most common operation you do when preparing a batch.
# `std::vector<async_io_op>` is very amenable to splitting the batch across threads (not that AFIO currently does this).
# `std::vector<async_io_op>` is easily transportable through an ABI, whereas arbitrary container iterators would need type
erasing (i.e. slow). As AFIO was developed initially as not header-only, this made a lot of sense initially.

We are not opposed to the use of generic iterator ranges in an AFIO v2 if there is user demand for such a thing.

[endsect] [/vector_use]

[section:foreign_fd How do I configure my own file descriptor or HANDLE in AFIO?]

Sometimes you receive a file descriptor or HANDLE from a third party piece of code and you need to insert it as-in
into AFIO for use. The solution to this is very simple:

# Subclass __afio_handle__ with a custom implementation for your file descriptor type. In particular, you probably
will have custom close semantics (e.g. don't close, or invoke third party code to implement close).
# Instantiate your custom handle implementation, and pass it into `async_file_io_dispatcher_base::adopt()`. This
will immediately convert your custom handle type into an `async_io_op` suitable for supplying to `read()`, `write()`
etc.
# That's it, there is no more to it.

[endsect] [/foreign_fd]

[endsect] [/FAQ]
