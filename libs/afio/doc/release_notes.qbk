[/============================================================================
  Boost.AFIO
  
  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:release_notes Release Notes]

[/=================]
[heading Boost 1.56 AFIO v1.1]
[/=================]

Immediate completions no longer hold the opslock, which meant the opslock could be
changed from a recursive mutex to a spinlock. The new, more parallelised, behaviour
illuminated a number of new race conditions in when_all() which have been fixed.

Completely gutted dispatch engine and replaced with a new, almost entirely wait
free implementation based on throwing atomics at the problem. If it weren't for the spin lock around the
central ops hash table, AFIO would now be an entirely wait free design.

In order to do something about that spin lock, replaced all locking in AFIO (apart
from the directory file handle cache) with memory transactions instead. This
does CPUID at runtime and will use Intel's TSX-NI memory transaction implementation
if available, if not it falls back to a spin lock based emulation. On memory
transaction capable CPUs, AFIO is now almost entirely wait free, apart from when
it has to fetch memory from the kernel.

Made AFIO usable as headers only.

[/=================]
[heading Boost 1.55 AFIO v1.0]
[/=================]

First release for end of Google Summer of Code 2013.

[endsect]




[section:FAQ Frequently Asked Questions]

[section:closure_performance What is the maximum throughput of AFIO's closure execution engine aka
how many IOPS can I push with AFIO?]

For v1.1, maximum ops [*dispatch] throughput is approximately as follows, where the values for `call()` might be for
use as a closure engine whereas the values for `completion()` might be for max filing system IOPS[footnote The phrase
["might be] is important: a null closure benchmark will always have dispatch rate problems i.e. the closures being executed
take less time to execute than the time to dispatch them, so these figures are best read as maximum dispatch rate, not
maximum IOPS.]:

[table:throughput Maximum null closure dispatch rate on a 3.5Ghz Intel Core i7 3770K for AFIO v1.1 i.e. NOT using transactional memory:
[[Operating system][`call()` unchained][`call()` chained][`completion()` unchained][`completion()` chained][Raw ASIO]]
[[Microsoft Windows 8 x64 with Visual Studio 2013][[role alignright 911963]][[role alignright 596318]][[role alignright 1555990]][[role alignright 726124]][[role alignright 2806070]]]
[[Relative to ASIO][[role alignright 32%]][[role alignright 21%]][[role alignright 55%]][[role alignright 26%]][[role alignright 100%]]]
[[Ubuntu 12.04 LTS Linux x64 with GCC 4.8][[role alignright 1094780]][[role alignright 794384]][[role alignright 1432810]][[role alignright 968005]][[role alignright 1611040[footnote Unfortunately the glibc mutex used by ASIO does not scale well on modern CPUs due to having too small a spin count, so this value is for four threads instead of eight (it was the best I could get through trial and error).]]]]
[[Relative to ASIO][[role alignright 68%]][[role alignright 49%]][[role alignright 89%]][[role alignright 60%]][[role alignright 100%]]]
[[Microsoft Windows 8 x64 with Visual Studio 2010][[role alignright 649015]][[role alignright 469689]][[role alignright 1021630]][[role alignright 613816]][[role alignright 2725690]]]
[[Relative to ASIO][[role alignright 24%]][[role alignright 17%]][[role alignright 37%]][[role alignright 23%]][[role alignright 100%]]]
]

We hope that ~720k max IOPS surely ought to be enough to max out any SATA III SSD -- it should even max out any mid range
PCIe based SSD too, and if paired with a CPU with more cores, AFIO's almost wait free design ought to scale out fairly
well too such that it may be able to max out even top end PCIe SSDs, some of which can push 10m IOPS now. If that isn't
fast enough for you, try AFIO on an Intel CPU with memory transaction support (on Windows a runtime check will automatically
replace locks with memory transactions, for POSIX you need to recompile with the `-mrtm` flag), or try support for `__transaction_relaxed` compilers if `BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER` is defined.

Nevertheless, AFIO will almost certainly be still criticised for poor performance relative to __boost_asio__. Unfortunately, dependency chaining is not
free, and moreover, AFIO has a very simple design based on a global hash table and pure C++11 STL throughout (i.e. it's heavy
on the memory allocator). The biggest limiter, by far, is the spin lock protecting the ops dispatch and completion implementation.
Because AFIO is pure batch, dispatch is very rarely a problem: rather it is completion, because completion handling must always be
performed per op completion in order to check for and dispatch, if necessary, dependencies. Currently the top consumers of CPU
time during null closure dispatch benchmarks are as follows:

[table:top_consumers Top consumers of CPU time during null closure dispatch in AFIO v1.1 on Microsoft Windows
[[Unchained][][Chained][]]
[[ops lock sleep][[role alignright 40%]][ops lock sleep][[role alignright 34%]]]
[[threadpool waiting for new ops][[role alignright 23%]][ops lock spin][[role alignright 22%]]]
[[ops lock spin][[role alignright 8.1%]][threadpool waiting for new ops][[role alignright 14%]]]
[[memory allocator][[role alignright 6.1%]][memory allocator][[role alignright 4.7%]]]
[[sending ops to threadpool][[role alignright 1.0%]][sending ops to threadpool][[role alignright 4.3%]]]
[[RtlUserThreadStart][[role alignright 10%]][RtlUserThreadStart][[role alignright 10%]]]
[[remainder and misc][[role alignright 11.8%]][remainder and misc][[role alignright 11%]]]
]

As you can see, when the closures do no real work, one spends a huge amount of time contending the ops lock.
Luckily, the ops lock is not held while file i/o operations or closures are being executed, so contention on the ops lock
isn't as much of a problem during real world use i.e. you will get a significant amount of ["free] file i/o without losing
much in maximum op dispatch throughput.

And certainly a custom allocator for the hash table might make quite a difference, especially in avoiding spinning and sleeping
the ops lock altogether by eliminating the memory allocator. Another option we might investigate in the future is using a
different hash table implementation e.g. `concurrent_unordered_map<>` instead (though I hear it has poor single lookup
performance, and the lack of safe erase is a huge problem for us), or any of the hash tables from
[@http://incise.org/hash-table-benchmarks.html].

[endsect] [/closure_performance]

[section:stuck_ops I'm seeing ["WARNING: `~async_file_dispatcher_base()` detects stuck `async_io_op` in total of X ops] during
process close. What does this mean?]

This means that you scheduled ops with a dispatcher which did not complete in a timely fashion before you tried to destroy
that dispatcher. This generally indicates a bug in your code, which can include:

# An op is still running in some thread pool and you didn't use __afio_when_all__ to wait for it to complete before trying
to destroy the dispatcher.
# An op's precondition never completed and therefore the op was never started.

Tracking down the cause of the latter in particular is very similar to tracking down race conditions i.e. it's hard, and we
as the authors of __boost_afio__ know just how hard! If you are on POSIX, recompiling AFIO with the macro
`BOOST_AFIO_OP_STACKBACKTRACEDEPTH` set to a reasonable depth like 8 will have AFIO take a stack backtrace for every op
allocated which it then will print during the stuck ops warnings. This can be helpful to identify which ops exactly are
stuck, and then you can figure out which preconditions of theirs are the cause.

Note that `BOOST_AFIO_OP_STACKBACKTRACEDEPTH` has two implementations, one based on glibc `backtrace()` and the other based
on libunwind. The latter is more portable, but requires explicit linking with libunwind, so we have defaulted to the former.

[endsect] [/stuck_ops]

[section:vector_use Why did you use `std::vector<>` as the ops batch container instead of a generic iterator range?]

# `std::vector<async_io_op>` is the closest thing to a variable length array in C++[footnote Ok, there is also
the oft-forgotten `std::valarray<>` too, but its use as a generic container isn't recommended.], at least until C++ 14 where we
will gain `std::dynarray<>` and dynamic array sizing.
# `std::vector<async_io_op>` is well understood, particularly its performance during by-value copies and during
`push_back()` which is by far the most common operation you do when preparing a batch.
# `std::vector<async_io_op>` is very amenable to splitting the batch across threads (not that AFIO currently does this).
# `std::vector<async_io_op>` is easily transportable through an ABI, whereas arbitrary container iterators would need type
erasing (i.e. slow). As AFIO was developed initially as not header-only, this made a lot of sense initially.

We are not opposed to the use of generic iterator ranges in an AFIO v2 if there is user demand for such a thing.

[endsect] [/vector_use]

[section:foreign_fd How do I configure my own file descriptor or HANDLE in AFIO?]

Sometimes you receive a file descriptor or HANDLE from a third party piece of code and you need to insert it as-in
into AFIO for use. The solution to this is very simple:

# Subclass __afio_handle__ with a custom implementation for your file descriptor type. In particular, you probably
will have custom close semantics (e.g. don't close, or invoke third party code to implement close).
# Instantiate your custom handle implementation, and pass it into `async_file_io_dispatcher_base::adopt()`. This
will immediately convert your custom handle type into an `async_io_op` suitable for supplying to `read()`, `write()`
etc.
# That's it, there is no more to it.

[endsect] [/foreign_fd]

[section:slow_compile Using AFIO really slows down my compile times. Can't you do something about that?]

We know about that slowdown! The solution is easy: define `BOOST_AFIO_HEADERS_ONLY` to 0 for each compiland, and
link to the AFIO shared library instead. Do the same for ASIO by defining `BOOST_ASIO_HEADER_ONLY` to 0 for
each compiland, and link to the ASIO shared library instead.

You should notice a 6x-10x improvement to compilation time, which is a huge win during debugging.

[endsect] [/slow_compile]

[endsect] [/FAQ]
