[/============================================================================
  Boost.AFIO
  
  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:release_notes Release Notes]

[/=================]
[heading Anticipated forthcoming features in future versions]
[/=================]

* Async fast batch hash engine which provides transparent hashing of all async
reads and writes (in progress).
  * New completion handler metaprogramming letting you bind add hash to each
op easily.
    * Automatic ASIO callback prototype spec support.

* Multiple dispatcher support
  * Integration of direct proposed Boost.Fiber library support as a secondary
thread_source to std_thread_source.

* async_io_dispatcher_base::read_partial() to read as much of a single buffer as
possible, rather than only complete buffers.

* Portable fast file locking which works across network shares, but can still
utilise shared memory when possible.

* Fast, scalable portable directory contents change monitoring. It should be able
to monitor a 1M entry directory experiencing 1% entry changes per second without
using a shocking amount of RAM.


[/=================]
[heading Boost 1.56 AFIO v1.20]
[/=================]

This is a major refactor of AFIO's core op dispatch engine to trim it down by
about 15%. Key breaking differences from the v1.1 series of AFIO are as follows:

* Replaced all use of packaged_task with enqueued_task, a custom implementation
which makes possible many performance improvements throughout the engine.
* thread_source::enqueue() now can take a preprepared enqueued_task.
* thread_source::enqueue() now always returns a shared_future instead of a future.
This has had knock on effects throughout AFIO, so many futures are now shared_future.
* Completion handler spec has changed from:
 
   pair<bool, shared_ptr<async_io_handle>> (*)(size_t id, shared_ptr<async_io_handle> h, exception_ptr *e)
 
  to:

   pair<bool, shared_ptr<async_io_handle>> (*)(size_t id, async_io_op preceding)
 
  This substantially improves performance, simplifies the implementation, and lets
  completion handlers more readily retrieve the error state of preceding operations
  and react appropriately.
* All restrictions on immediate completions have been removed. You can now do anything
in an immediate completion that you can do in a normal completion.
* async_io_op::h now always refers to a correct future i.e. the future is no longer
lazily allocated.
* Now that op futures are always correct, when_all(ops) has been drastically simplified
to an implementation which literally assembles the futures into a list and passes
them to boost::wait_for_all().
* Added when_any(ops).


[/=================]
[heading Boost 1.56 AFIO v1.11]
[/=================]

Added --fast-build to test Jamfile to preserve my sanity attempting to work with
AFIO on an Intel Atom 220 netbook.

Fixed failure to auto-const an async_data_op_req<boost::asio::mutable_buffer>
when used for writing. Thanks to Bjorn Reese for reporting this.

Replaced use of std::runtime_error with std::invalid_argument where that makes
sense. Thanks to Bjorn Reese for reporting this.

Replaced throwing of std::ios_base::failure with std::system_error. Thanks to
Bjorn Reese for suggesting and submitting a patch for this.

async_io_dispatcher_base::enumerate() did not take a metadata_flags, and it
was supposed to. Thanks to Bjorn Reese for reporting this.

Added a unit compilation test to ensure that implicit construction from a
single arg to the op convenience classes works as intended.

Significantly optimised build system and added in precompiled headers support.
Combined with --fast-build this provides an 8x build time improvement.

boost::afio::stat_t::st_type() is now a boost::filesystem::file_type instead
of replicating the POSIX file type codes. Thanks to Bjorn Reese for suggesting
this.

boost::afio::stat_t::st_mode() is now st_perms(). Also disabled unused fields in
stat_t on Windows. Thanks to Bjorn Reese for suggesting this.

[/=================]
[heading Boost 1.55 AFIO v1.1]
[/=================]

Immediate completions no longer hold the opslock, which meant the opslock could be
changed from a recursive mutex to a spinlock. The new, more parallelised, behaviour
illuminated a number of new race conditions in when_all() which have been fixed.

Completely gutted dispatch engine and replaced with a new, almost entirely wait
free implementation based on throwing atomics at the problem. If it weren't for the spin lock around the
central ops hash table, AFIO would now be an entirely wait free design.

In order to do something about that spin lock, replaced all locking in AFIO (apart
from the directory file handle cache) with memory transactions instead. This
does CPUID at runtime and will use Intel's TSX-NI memory transaction implementation
if available, if not it falls back to a spin lock based emulation. On memory
transaction capable CPUs, AFIO is now almost entirely wait free, apart from when
it has to fetch memory from the kernel.

Made AFIO usable as headers only.

[/=================]
[heading Boost 1.55 AFIO v1.0]
[/=================]

First release for end of Google Summer of Code 2013.

[endsect]




[section:FAQ Frequently Asked Questions]

[section:closure_performance What is the maximum throughput of AFIO's closure execution engine aka
how many IOPS can I push with AFIO?]

For v1.1, maximum ops [*dispatch] throughput is approximately as follows, where the values for `call()` might be for
use as a closure engine whereas the values for `completion()` might be for max filing system IOPS[footnote The phrase
["might be] is important: a null closure benchmark will always have dispatch rate problems i.e. the closures being executed
take less time to execute than the time to dispatch them, so these figures are best read as maximum dispatch rate, not
maximum IOPS.]:

[table:throughput Maximum null closure dispatch rate on a 3.5Ghz Intel Core i7 3770K for AFIO v1.1 i.e. NOT using transactional memory:
[[Operating system][`call()` unchained][`call()` chained][`completion()` unchained][`completion()` chained][Raw ASIO]]
[[Microsoft Windows 8 x64 with Visual Studio 2013][[role alignright 911963]][[role alignright 596318]][[role alignright 1555990]][[role alignright 726124]][[role alignright 2806070]]]
[[Relative to ASIO][[role alignright 32%]][[role alignright 21%]][[role alignright 55%]][[role alignright 26%]][[role alignright 100%]]]
[[Ubuntu 12.04 LTS Linux x64 with GCC 4.8][[role alignright 1094780]][[role alignright 794384]][[role alignright 1432810]][[role alignright 968005]][[role alignright 1611040[footnote Unfortunately the glibc mutex used by ASIO does not scale well on modern CPUs due to having too small a spin count, so this value is for four threads instead of eight (it was the best I could get through trial and error).]]]]
[[Relative to ASIO][[role alignright 68%]][[role alignright 49%]][[role alignright 89%]][[role alignright 60%]][[role alignright 100%]]]
[[Microsoft Windows 8 x64 with Visual Studio 2010][[role alignright 649015]][[role alignright 469689]][[role alignright 1021630]][[role alignright 613816]][[role alignright 2725690]]]
[[Relative to ASIO][[role alignright 24%]][[role alignright 17%]][[role alignright 37%]][[role alignright 23%]][[role alignright 100%]]]
]

We hope that ~720k max IOPS surely ought to be enough to max out any SATA III SSD __dash__ it should even max out any mid range
PCIe based SSD too, and if paired with a CPU with more cores, AFIO's almost wait free design ought to scale out fairly
well too such that it may be able to max out even top end PCIe SSDs, some of which can push 10m IOPS now. If that isn't
fast enough for you, try AFIO on an Intel CPU with memory transaction support (a runtime check will automatically
replace locks with TSX memory transactions), or try support for `__transaction_relaxed` compilers if `BOOST_HAVE_TRANSACTIONAL_MEMORY_COMPILER` is defined.

Nevertheless, AFIO will almost certainly be still criticised for poor performance relative to __boost_asio__. Unfortunately, dependency chaining is not
free, and moreover, AFIO has a very simple design based on a global hash table and pure C++11 STL throughout (i.e. it's heavy
on the memory allocator). The biggest limiter, by far, is the spin lock protecting the ops dispatch and completion implementation.
Because AFIO is pure batch, dispatch is very rarely a problem: rather it is completion, because completion handling must always be
performed per op completion in order to check for and dispatch, if necessary, dependencies. Currently the top consumers of CPU
time during null closure dispatch benchmarks are as follows:

[table:top_consumers Top consumers of CPU time during null closure dispatch in AFIO v1.1 on Microsoft Windows
[[Unchained][][Chained][]]
[[ops lock sleep][[role alignright 40%]][ops lock sleep][[role alignright 34%]]]
[[threadpool waiting for new ops][[role alignright 23%]][ops lock spin][[role alignright 22%]]]
[[ops lock spin][[role alignright 8.1%]][threadpool waiting for new ops][[role alignright 14%]]]
[[memory allocator][[role alignright 6.1%]][memory allocator][[role alignright 4.7%]]]
[[sending ops to threadpool][[role alignright 1.0%]][sending ops to threadpool][[role alignright 4.3%]]]
[[RtlUserThreadStart][[role alignright 10%]][RtlUserThreadStart][[role alignright 10%]]]
[[remainder and misc][[role alignright 11.8%]][remainder and misc][[role alignright 11%]]]
]

As you can see, when the closures do no real work, one spends a huge amount of time contending the ops lock.
Luckily, the ops lock is not held while file i/o operations or closures are being executed, so contention on the ops lock
isn't as much of a problem during real world use i.e. you will get a significant amount of ["free] file i/o without losing
much in maximum op dispatch throughput.

And certainly a custom allocator for the hash table might make quite a difference, especially in avoiding spinning and sleeping
the ops lock altogether by eliminating the memory allocator. Another option we might investigate in the future is using a
different hash table implementation e.g. `concurrent_unordered_map<>` instead (though I hear it has poor single lookup
performance, and the lack of safe erase is a huge problem for us), or any of the hash tables from
[@http://incise.org/hash-table-benchmarks.html].

[endsect] [/closure_performance]

[section:stuck_ops I'm seeing ["WARNING: `~async_file_dispatcher_base()` detects stuck `async_io_op` in total of X ops] during
process close. What does this mean?]

This means that you scheduled ops with a dispatcher which did not complete in a timely fashion before you tried to destroy
that dispatcher. This generally indicates a bug in your code, which can include:

# An op is still running in some thread pool and you didn't use __afio_when_all__ to wait for it to complete before trying
to destroy the dispatcher.
# An op's precondition never completed and therefore the op was never started.

Tracking down the cause of the latter in particular is very similar to tracking down race conditions i.e. it's hard, and we
as the authors of __boost_afio__ know just how hard! If you are on POSIX, recompiling AFIO with the macro
`BOOST_AFIO_OP_STACKBACKTRACEDEPTH` set to a reasonable depth like 8 will have AFIO take a stack backtrace for every op
allocated which it then will print during the stuck ops warnings. This can be helpful to identify which ops exactly are
stuck, and then you can figure out which preconditions of theirs are the cause.

Note that `BOOST_AFIO_OP_STACKBACKTRACEDEPTH` has two implementations, one based on glibc `backtrace()` and the other based
on libunwind. The latter is more portable, but requires explicit linking with libunwind, so we have defaulted to the former.

[endsect] [/stuck_ops]

[section:vector_use Why did you use `std::vector<>` as the ops batch container instead of a generic iterator range?]

# `std::vector<async_io_op>` is the closest thing to a variable length array in C++[footnote Ok, there is also
the oft-forgotten `std::valarray<>` too, but its use as a generic container isn't recommended.], at least until C++ 14 where we
will gain `std::dynarray<>` and dynamic array sizing.
# `std::vector<async_io_op>` is well understood, particularly its performance during by-value copies and during
`push_back()` which is by far the most common operation you do when preparing a batch.
# `std::vector<async_io_op>` is very amenable to splitting the batch across threads (not that AFIO currently does this).
# `std::vector<async_io_op>` is easily transportable through an ABI, whereas arbitrary container iterators would need type
erasing (i.e. slow). As AFIO was developed initially as not header-only, this made a lot of sense initially.

We are not opposed to the use of generic iterator ranges in an AFIO v2 if there is user demand for such a thing.

[endsect] [/vector_use]

[section:foreign_fd How do I configure my own file descriptor or HANDLE in AFIO?]

Sometimes you receive a file descriptor or HANDLE from a third party piece of code and you need to insert it as-in
into AFIO for use. The solution to this is very simple:

# Subclass __afio_handle__ with a custom implementation for your file descriptor type. In particular, you probably
will have custom close semantics (e.g. don't close, or invoke third party code to implement close).
# Instantiate your custom handle implementation, and pass it into `async_file_io_dispatcher_base::adopt()`. This
will immediately convert your custom handle type into an `async_io_op` suitable for supplying to `read()`, `write()`
etc.
# That's it, there is no more to it.

[endsect] [/foreign_fd]

[section:slow_compile Using AFIO really slows down my compile times. Can't you do something about that?]

We know about that slowdown! The solution is easy: define `BOOST_AFIO_HEADERS_ONLY` to 0 for each compiland, and
link to the AFIO shared library instead. Do the same for ASIO by defining `BOOST_ASIO_SEPARATE_COMPILATION`
or `BOOST_ASIO_DYN_LINK` as is appropriate to your circumstances for each compiland, and link to the ASIO
shared library instead (which is built via `boost/asio/impl/src.hpp`, see ASIO's docs).

While those alone are a significant improvement, still further improvements can be gained by
configuring precompiled headers support for your compiler. Getting this to work correctly is, as
always, quite brittle but is very much worth it on low end CPUs. The Jamfile.v2 in the test
directory implements an option `--fast-build` which is an example of how to achieve maximum
build performance with AFIO.

You should notice a 6x-10x improvement to compilation time, which is a huge win during debugging, or
building on low end CPUs like first gen Intel Atom's.

[endsect] [/slow_compile]

[section:fatal_error_read Why do I get a fatal application exit with `FATAL EXCEPTION: Failed to read all buffers` when I read a file?]

This is actually a safety checkpoint for your code: in complex, multi-process concurrent reading and writing of
the same file, it is extremely difficult to coordinate changing file lengths with i/o in a way which
doesn't introduce race conditions OR unacceptably low performance. AFIO therefore doesn't even
try[footnote AFIO [*will] try to provide a synchronised, accurate file extent after fast portable
file locking support has been added, but until then no.]
and simply requires you the programmer to ALWAYS do i/o, whether reading or writing, within the extent
of a file. In other words, if you're going to read 100 bytes from offset 100 in a file, that file
better be at least 200 bytes long or it's going to fail with a fatal application exit.

This will probably seem harsh to anyone using AFIO for the first time, because the following
naive code will fatal exit the application if foo.txt is not 1024 bytes or longer:

[readallof_example_bad]

With synchronous i/o a read of 1024 bytes will read ['up to] 1024 bytes, returning the amount
actually read via some mechanism. With AFIO, either you
read [*all] 1024 bytes or you read nothing, in which case a normal exception is thrown with
whatever error the operating system returned. If a ['partial] read happens, then AFIO fatal
exits the application with the above message as it is probably a logic error in your code.

You may now wonder how to easily not exceed file extents during i/o: for writing, see
__afio_truncate__ to ensure a file's size before writing. For reading, the following code
is suggested:

[readallof_example_single]

If you're going to read many files from the same directory, it is far faster to open a
handle to the containing directory and using enumerate to fetch the metadata asynchronously
instead of using `direntry()` which is synchronous:

[readallof_example_many]

[endsect] [/fatal_error_read]

[section:async_metadata `async_io_handle::direntry()` and `async_io_handle::lstat()` are
both synchronous functions which block. How then can I get metadata about files and
directories asynchronously?]

This is easy, if not terribly obvious: call __afio_enumerate__ on the containing
directory of the file you want metadata for with a shell glob exactly matching the
file name and with the minimum metadata you are looking for. AFIO will then
asynchronously fetch that metadata for you, returning it in the future `directory_entry`
returned by `enumerate()`.

[endsect] [/async_metadata]

[endsect] [/FAQ]
